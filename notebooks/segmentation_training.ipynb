{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNによる細胞セグメンテーション\n",
    "\n",
    "画像セグメンテーションとは、画像内における特定のオブジェクトを区画化する画像処理を指す。\n",
    "ここでは、画像内の細胞領域をセグメンテーションするタスクを学習するためのCNNを構築し、訓練および評価までの一連の作業を行う。\n",
    "\n",
    "\n",
    "## データセット\n",
    "\n",
    "`dataset_seg/raw` にHeLa細胞の位相差顕微鏡画像18枚, `dataset_seg/gt`に細胞セグメンテーション画像18枚が格納されている。  \n",
    "画像サイズはすべて 512x512 pixel で、グレースケール画像となっている。  \n",
    "\n",
    "まず最初に画像を読み込み表示してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "img_raw = io.imread('../datasets/dataset_seg/raw/001.tif')\n",
    "io.imshow(img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "img_gt = io.imread('../datasets/dataset_seg/gt/001.tif')\n",
    "io.imshow(img_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import chainer\n",
    "\n",
    "class SegmentationDataset(chainer.dataset.DatasetMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_path,\n",
    "            split_list\n",
    "    ):\n",
    "        self.root_path = root_path\n",
    "        with open(split_list) as f:\n",
    "            self.split_list = [line.rstrip() for line in f]\n",
    "        self.dtype = np.float32\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_list)\n",
    "\n",
    "    def _get_image(self, i):\n",
    "        image = io.imread(os.path.join(self.root_path, 'raw', self.split_list[i]))\n",
    "        image = self._min_max_normalize_one_image(image)\n",
    "        return np.expand_dims(image.astype(self.dtype), axis=0)\n",
    "\n",
    "    def _min_max_normalize_one_image(self, image):\n",
    "        max_int = image.max()\n",
    "        min_int = image.min()\n",
    "        out = (image.astype(np.float32) - min_int) / (max_int - min_int)\n",
    "        return out\n",
    "    \n",
    "    def _get_label(self, i):\n",
    "        label = io.imread(os.path.join(self.root_path, 'gt', self.split_list[i])) / 255\n",
    "        return label.astype(np.int32)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        x, y = self._get_image(i), self._get_label(i)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "class SegmentationModel(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_class=2):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            # L.Convolution2D(in_ch, out_ch, ksize, stride, pad)\n",
    "            self.conv1_1 = L.Convolution2D(None, 16, ksize=3, stride=1, pad=1)\n",
    "            self.conv1_2 = L.Convolution2D(None, 16, ksize=3, stride=1, pad=1)\n",
    "            self.conv2_1 = L.Convolution2D(None, 32, ksize=3, stride=1, pad=1)\n",
    "            self.conv2_2 = L.Convolution2D(None, 32, ksize=3, stride=1, pad=1)\n",
    "            self.conv3_1 = L.Convolution2D(None, 64, ksize=3, stride=1, pad=1)\n",
    "            self.conv3_2 = L.Convolution2D(None, 64, ksize=3, stride=1, pad=1)\n",
    "            self.conv4_1 = L.Convolution2D(None, 32, ksize=3, stride=1, pad=1)\n",
    "            self.conv4_2 = L.Convolution2D(None, 32, ksize=3, stride=1, pad=1)\n",
    "            self.conv5_1 = L.Convolution2D(None, 16, ksize=3, stride=1, pad=1)\n",
    "            self.conv5_2 = L.Convolution2D(None, 16, ksize=3, stride=1, pad=1)\n",
    "            self.conv6 = L.Convolution2D(None, n_class, ksize=1, stride=1, pad=0)\n",
    "\n",
    "            # L.Deconvolution2D(in_ch, out_ch, ksize, stride, pad)\n",
    "            self.dconv1 = L.Deconvolution2D(None, 32, ksize=2, stride=2, pad=0)\n",
    "            self.dconv2 = L.Deconvolution2D(None, 16, ksize=2, stride=2, pad=0)\n",
    "\n",
    "            self.bn1_1 = L.BatchNormalization(16)\n",
    "            self.bn1_2 = L.BatchNormalization(16)\n",
    "            self.bn2_1 = L.BatchNormalization(32)\n",
    "            self.bn2_2 = L.BatchNormalization(32)\n",
    "            self.bn3_1 = L.BatchNormalization(64)\n",
    "            self.bn3_2 = L.BatchNormalization(64)\n",
    "            self.bn4_1 = L.BatchNormalization(32)\n",
    "            self.bn4_2 = L.BatchNormalization(32)\n",
    "            self.bn5_1 = L.BatchNormalization(16)\n",
    "            self.bn5_2 = L.BatchNormalization(16)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        e1 = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        e2 = F.relu(self.bn1_2(self.conv1_2(e1)))\n",
    "        del e1\n",
    "        e3 = F.max_pooling_2d(e2, 2, 2)\n",
    "        e4 = F.relu(self.bn2_1(self.conv2_1(e3)))\n",
    "        e5 = F.relu(self.bn2_2(self.conv2_2(e4)))\n",
    "        del e3, e4\n",
    "        e6 = F.max_pooling_2d(e5, 2, 2)\n",
    "        e7 = F.relu(self.bn3_1(self.conv3_1(e6)))\n",
    "        e8 = F.relu(self.bn3_2(self.conv3_2(e7)))\n",
    "        del e6, e7\n",
    "        d1 = F.concat([self.dconv1(e8), e5])\n",
    "        del e5, e8\n",
    "        d2 = F.relu(self.bn4_1(self.conv4_1(d1)))\n",
    "        d3 = F.relu(self.bn4_2(self.conv4_2(d2)))\n",
    "        del d1, d2\n",
    "        d4 = F.concat([self.dconv2(d3), e2])\n",
    "        del e2, d3\n",
    "        d5 = F.relu(self.bn5_1(self.conv5_1(d4)))\n",
    "        d6 = F.relu(self.bn5_2(self.conv5_2(d5)))\n",
    "        del d4, d5\n",
    "        d7 = self.conv6(d6)\n",
    "        del d6\n",
    "        return d7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数の定義\n",
    "\n",
    "分類問題のときと同様に、Softmax Cross Entropy 関数を目的関数とする。\n",
    "\n",
    "\n",
    "## 評価\n",
    "\n",
    "一般的に、セグメンテーションの精度は**Pixel accuracy**や、**Mean Intersection over Union (mIoU)**といった指標で評価される。  \n",
    "正解クラスが$i$であるピクセルをCNNモデルがクラス$j$に分類した数を$N_{ij}$とすると、クラス数が$k$のとき、Pixel AccuracyとmIoUは以下のようになる。\n",
    "\n",
    "$$\n",
    "{\\rm Pixel\\ Accuracy} = \\frac{\\sum_{i=1}^k N_{ii}}{\\sum_{i=1}^k \\sum_{j=1}^k N_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm mIoU} = \\frac{1}{k} \\sum_{i=1}^k \\frac{N_{ii}}{\\sum_{j=1}^k N_{ij} + \\sum_{j=1}^k N_{ji} - N_{ii}}\n",
    "$$\n",
    "\n",
    "セグメンテーション精度の指標であるPixel AccuracyやmIoUを用いる場合、`chainercv` の `evaluations.eval_semantic_segmentation` 利用する。\n",
    "\n",
    "参考: https://chainercv.readthedocs.io/en/stable/reference/evaluations.html#semantic-segmentation-iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "from chainer import reporter, Variable, cuda\n",
    "from chainercv import evaluations\n",
    "\n",
    "class Classifier(chainer.Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(Classifier, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # 学習対象のモデルをpredictorとして保持しておく\n",
    "            self.predictor = predictor\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        y = self.predictor(x)\n",
    "        loss = F.softmax_cross_entropy(y, t)\n",
    "        with chainer.no_backprop_mode():\n",
    "            y = F.softmax(y).data\n",
    "            y = np.array(y[:, 1, ...] > y[:, 0, ...], dtype=np.int32)\n",
    "            evals = evaluations.eval_semantic_segmentation(y, t)\n",
    "        reporter.report({'loss': loss}, self)\n",
    "        reporter.report({'accuracy': evals['pixel_accuracy']}, self)\n",
    "        reporter.report({'miou': evals['miou']}, self)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        with chainer.function.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            x = Variable(self.xp.asarray(x, dtype=self.xp.float32))\n",
    "            y = self.predictor(x)\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainerの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "from chainer import optimizers, iterators, training\n",
    "\n",
    "def create_trainer(root_path, train_path, val_path, batchsize, epoch, out='result_seg', device=-1):\n",
    "\n",
    "    train_dataset = SegmentationDataset(root_path, train_path)\n",
    "    val_dataset = SegmentationDataset(root_path, val_path)\n",
    "    \n",
    "    model = SegmentationModel(n_class=2)\n",
    "    train_model = Classifier(model)\n",
    "\n",
    "    optimizer = optimizers.Adam()\n",
    "    optimizer.setup(train_model)\n",
    "\n",
    "    train_iter = iterators.MultiprocessIterator(train_dataset, batchsize)\n",
    "    val_iter = iterators.MultiprocessIterator(val_dataset, batchsize, repeat=False, shuffle=False)\n",
    "\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=device)\n",
    "\n",
    "    trainer = training.trainer.Trainer(updater, epoch, out=out)\n",
    "\n",
    "    logging_attributes = [\n",
    "        'epoch', 'main/loss', 'main/accuracy', 'main/miou',\n",
    "        'val/main/loss', 'val/main/accuracy', 'val/main/miou']\n",
    "    trainer.extend(training.extensions.LogReport(logging_attributes))\n",
    "    trainer.extend(training.extensions.PrintReport(logging_attributes))\n",
    "    trainer.extend(training.extensions.PlotReport(['main/loss', 'val/main/loss'], 'epoch', file_name='loss.png'))\n",
    "    trainer.extend(training.extensions.PlotReport(['main/accuracy', 'val/main/accuracy'], 'epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(training.extensions.PlotReport(['main/miou', 'val/main/miou'], 'epoch', file_name='miou.png'))\n",
    "    trainer.extend(training.extensions.Evaluator(val_iter, train_model, device=device), name='val')\n",
    "    trainer.extend(training.extensions.snapshot_object(train_model, filename='best_miou_model'), trigger=training.triggers.MaxValueTrigger('val/main/miou', trigger=(1, 'epoch')))    \n",
    "    trainer.extend(training.extensions.dump_graph('main/loss'))\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.makedirs('results', exist_ok=True)\n",
    "root_path = '../datasets/dataset_seg'\n",
    "train_path = '../datasets/dataset_seg/split_list/train.txt'\n",
    "val_path = '../datasets/dataset_seg/split_list/validation.txt'\n",
    "\n",
    "batchsize = 2\n",
    "epoch = (5, 'epoch')\n",
    "\n",
    "trainer = create_trainer(root_path, train_path, val_path, batchsize, epoch, out='results/result_seg', device=-1)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../datasets/dataset_seg'\n",
    "batchsize = 2\n",
    "epoch = (10, 'epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_fold1 = '../datasets/dataset_seg/split_list/fold1/train.txt'\n",
    "val_path_fold1 = '../datasets/dataset_seg/split_list/fold1/validation.txt'\n",
    "\n",
    "trainer_fold1 = create_trainer(root_path, train_path_fold1, val_path_fold1, batchsize, epoch=epoch, out='results/result_seg_fold1', device=-1\n",
    "trainer_fold1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_fold2 = '../datasets/dataset_seg/split_list/fold2/train.txt'\n",
    "val_path_fold2 = '../datasets/dataset_seg/split_list/fold2/validation.txt'\n",
    "\n",
    "trainer_fold2 = create_trainer(root_path, train_path_fold2, val_path_fold2, batchsize, epoch=epoch, out='results/result_seg_fold2', device=-1)\n",
    "trainer_fold2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_fold3 = '../datasets/dataset_seg/split_list/fold3/train.txt'\n",
    "val_path_fold3 = '../datasets/dataset_seg/split_list/fold3/validation.txt'\n",
    "\n",
    "trainer_fold3 = create_trainer(root_path, train_path_fold3, val_path_fold3, batchsize, epoch=epoch, out='results/result_seg_fold3', device=-1)\n",
    "trainer_fold3.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_fold4 = '../datasets/dataset_seg/split_list/fold4/train.txt'\n",
    "val_path_fold4 = '../datasets/dataset_seg/split_list/fold4/validation.txt'\n",
    "\n",
    "trainer_fold4 = create_trainer(root_path, train_path_fold4, val_path_fold4, batchsize, epoch=epoch, out='results/result_seg_fold4', device=-1)\n",
    "trainer_fold4.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fold5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_fold5 = '../datasets/dataset_seg/split_list/fold5/train.txt'\n",
    "val_path_fold5 = '../datasets/dataset_seg/split_list/fold5/validation.txt'\n",
    "\n",
    "trainer_fold5 = create_trainer(root_path, train_path_fold5, val_path_fold5, batchsize, epoch=epoch, out='results/result_seg_fold5', device=-1)\n",
    "trainer_fold5.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validationの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "res_cv_path = glob.glob('results/result_seg_fold*')\n",
    "res_log = []\n",
    "for cv in res_cv_path:\n",
    "    with open(os.path.join(cv, 'log'), 'r') as f:\n",
    "        res_log.append(json.load(f))\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "plt.figure(figsize=(18, 10))\n",
    "for k in range(len(res_log)):\n",
    "    train, val, epoch = [], [], []\n",
    "    for n in range(len(res_log[k])):\n",
    "        train.append(res_log[k][n]['main/accuracy'])\n",
    "        val.append(res_log[k][n]['val/main/accuracy'])\n",
    "        epoch.append(res_log[k][n]['epoch'])\n",
    "    plt.subplot(2, 3, k+1)\n",
    "    plt.plot(epoch, train, label='Train', color='blue')\n",
    "    plt.plot(epoch, val, label='Validation', color='orange')\n",
    "    plt.scatter(np.argmax(val)+1, np.max(val), marker='o', color='orange')\n",
    "    plt.xticks(epoch)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('fold{}: max accuracy={}'.format(k+1, np.max(val)))\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(18, 10))\n",
    "for k in range(len(res_log)):\n",
    "    train, val, epoch = [], [], []\n",
    "    for n in range(len(res_log[k])):\n",
    "        train.append(res_log[k][n]['main/miou'])\n",
    "        val.append(res_log[k][n]['val/main/miou'])\n",
    "        epoch.append(res_log[k][n]['epoch'])\n",
    "    plt.subplot(2, 3, k+1)\n",
    "    plt.plot(epoch, train, label='Train', color='blue')\n",
    "    plt.plot(epoch, val, label='Validation', color='orange')\n",
    "    plt.scatter(np.argmax(val)+1, np.max(val), marker='o', color='orange')\n",
    "    plt.xticks(epoch)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('mIoU')\n",
    "    plt.title('fold{}: max mIoU={}'.format(k+1, np.max(val)))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト関数の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "import chainer.functions as F\n",
    "from chainer import serializers\n",
    "from chainercv import evaluations\n",
    "\n",
    "def test(root_path, test_path, model_path, out='results/result_seg_test', device=-1):\n",
    "    test_dataset = SegmentationDataset(root_path, test_path)\n",
    "    \n",
    "    model = SegmentationModel(n_class=2)\n",
    "    train_model = Classifier(model)\n",
    "    serializers.load_npz(model_path, train_model)\n",
    "    \n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    accuracy, miou = 0, 0\n",
    "    for i in range(test_dataset.__len__()):\n",
    "        with chainer.using_config('train', False):\n",
    "            x, t = test_dataset.get_example(i)\n",
    "            y = train_model.predict(np.expand_dims(x, axis=0))\n",
    "            y = F.softmax(y).data\n",
    "        y = np.array(y[:, 1, ...] > y[:, 0, ...], dtype=np.int32)\n",
    "        t = np.expand_dims(t, axis=0)\n",
    "        evals = evaluations.eval_semantic_segmentation(y, t)\n",
    "        accuracy += evals['pixel_accuracy']\n",
    "        miou += evals['miou']\n",
    "        io.imsave(os.path.join(out, test_dataset.split_list[i]), np.array(y[0], dtype=np.uint8) * 255)\n",
    "                \n",
    "    with open(os.path.join(out, 'result.txt'), 'w') as f:\n",
    "        f.write('accuracy: {}\\n'.format(accuracy / test_dataset.__len__()))\n",
    "        f.write('mIoU: {}\\n'.format(miou/ test_dataset.__len__()))\n",
    "    print('accuracy: {}'.format(accuracy / test_dataset.__len__()))\n",
    "    print('mIoU: {}'.format(miou / test_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../datasets/dataset_seg'\n",
    "test_path = '../datasets/dataset_seg/split_list/test.txt'\n",
    "model_path = 'results/result_seg/best_miou_model'\n",
    "\n",
    "test(root_path, test_path, model_path, 'results/result_seg_test', device=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
